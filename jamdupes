#!/usr/bin/env python3
# -*- coding: utf-8 -*-

__author__ = "Jim Male <jmale1@gmail.com>"
__copyright__ = "Copyright (c) 2016"
__credits__ = ["Jim Male <jmale1@gmail.com>"]
__license__ = "BSD-3"


import os
import sys
import argparse
import hashlib

def index(path, db):
	filesize = os.path.getsize(path)

	if filesize in db.keys():
		db[filesize].append(path)
	else:
		db[filesize] = [path]

def walk(path, db, symlinks):
	filesToIndex = []
	for root, dirs, files in os.walk(path, followlinks=symlinks):
		for name in files:
				filepath = os.path.abspath(os.path.join(root, name))
				filesToIndex.append(filepath)

	## this avoids double-hashing files that might be in folders specified twice
	## eg if you recursively scan both /home/ and /home/jim
	filesToIndex = set(filesToIndex)
	for filepath in filesToIndex:
		index(filepath, db)

## Trims a dictionary. Any key that corresponds to one (and only one) value is deleted.
def trimDb(db):
	keysToDelete = []
	for key in db.keys():
		if len(db[key]) <= 1:
			keysToDelete.append(key)
	for key in keysToDelete:
		del db[key]


def hashChunk(path, chunkSize):
	#print("Chunk: " + path)
	f = open(path, "rb")
	result = hashlib.sha1(f.read(chunkSize)).hexdigest()
	f.close()
	return result

def hashWholeFile(path, chunkSize):
	#print("File: " + path)
	f = open(path, "rb")
	m = hashlib.sha1()
	while True:
		buff=f.read(chunkSize)
		if not buff:
			break
		else:
			m.update(buff)
	f.close()
	return m.hexdigest()

if __name__ == "__main__":
	try:
		parser = argparse.ArgumentParser(prog='jamdupes')
		parser.add_argument(dest='nonrecurseDirectories', default=[], action='append', nargs='*', metavar="DIRECTORY")
		parser.add_argument('-n', '--noempty', dest='ignoreZeros', action='store_const', const=True, default=False, help='ignore zero-length files')

		parser.add_argument('-1', '--sameline', dest='omitfirst', action='store_const', const=True, default=False, help='list all matching files on one line, separated by spaces.')
		parser.add_argument('-f', '--omitfirst', dest='omitfirst', action='store_const', const=True, default=False, help='omit the first copy of a file found. This may be a random file from the set of matching files.')
		parser.add_argument('-s', '--symlinks', dest='followSymlinks', action='store_const', const=True, default=False, help='follow symlinks')
		parser.add_argument('-S', '--size', dest='showSizeOfFiles', action='store_const', const=True, default=False, help='show size of duplicate files')
		parser.add_argument('-R', '--recurse:', dest='recurseDirectories', default=[], action='append', nargs='+', metavar='DIRECTORY', help="recurse into *all* following directories")
		parser.add_argument('-r', '--recurse', dest='recurseDirectories', default=[], action='append', nargs=1, metavar='DIRECTORY', help="recurse into the following directory")
		parser.add_argument('-q', '--quiet', dest='quiet', default=False, action='store_const', const=True, help="hide progress bar")

		parser.add_argument('-c', '--chunk', dest='chunk', default=10000000, action='store_const', const=True, help='initial test chunk size for large files in bytes. default: 10MB')
		args = parser.parse_args()

		## flattening the directory lists. code from 
		## http://stackoverflow.com/questions/11264684/flatten-list-of-lists
		## thanks to Paul Seeb for this snippet.
		args.recurseDirectories = [val for sublist in args.recurseDirectories for val in sublist]
		args.nonrecurseDirectories = [val for sublist in args.nonrecurseDirectories for val in sublist]
		## end snippet


		db = {}
		for dir in args.recurseDirectories:
			print("Indexing " + dir + "\t", end="", flush=True)
			walk(dir, db, args.followSymlinks)
			print("done.\n")

		for dir in args.nonrecurseDirectories:
			print("Indexing " + dir + "\t", end="",  flush=True)
			for file in os.listdir(dir):
				if os.path.isfile(os.path.join(dir, file)):
					filepath = os.path.join(dir, file)
					index(filepath, db)
			print("done.\n")

		trimDb(db)

		if(args.ignoreZeros):
			if 0 in db.keys():
				del db[0]

		bytecount = 0
		for key in db.keys():
			bytecount=bytecount+len(db[key])*key
		
		bytesProcessed = 0

		hashes = {}

		for size in db.keys():
			if(size < args.chunk):
				for file in db[size]:
					hash = hashWholeFile(file, args.chunk)
					if hash in hashes.keys():
						hashes[hash].append(file)
					else:
						hashes[hash] = [file]
					bytesProcessed = bytesProcessed + size
			else:
				tmp = {}
				for file in db[size]:
					hash = hashChunk(file, args.chunk)
					if hash in tmp.keys():
						tmp[hash].append(file)
					else:
						tmp[hash] = [file]
					bytesProcessed = bytesProcessed + size
				trimDb(tmp)
				for key in tmp.keys():
					for file in tmp[key]:
						hash = hashWholeFile(file, args.chunk)
						if hash in hashes.keys():
							hashes[hash].append(file)
						else:
							hashes[hash] = [file]
						bytesProcessed = bytesProcessed + size
			print("Progress: " + str((bytesProcessed*1.0/bytecount*1.0)*100)[0:6] + "%")

		trimDb(hashes)
		for hash in hashes:
			if(args.showSizeOfFiles):
				print(os.path.getsize(hashes[hash][0]))
			for file in hashes[hash]:
				print(file)
			print()	
	except (KeyboardInterrupt):
		print("", end="")
